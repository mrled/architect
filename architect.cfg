---

##
#### User-managed variables
##

# Your AWS region
aws_region: us-east-2

# The bucket we use during deployments
deploy_bucket:
    name: third-jersey-deployments
    arn: arn:aws:s3:::third-jersey-deployments

# The VPN host - either an IP address or a domain name
algo_vpn_host: newtroy.micahrl.com

# The parameterized contents of Algo's 'ipsec_USER.secrets' file:
algo_ipsec_secrets: "{{ algo_vpn_host }} : ECDSA architect.key"

# The contents of Algo's ipsec_USER.conf file
# We parameterize this where we can, and make a couple of changes, documented in the comments
algo_ipsec_conf: |+
    conn ikev2-{{ algo_vpn_host }}
        fragmentation=yes
        rekey=no
        keyexchange=ikev2
        compress=no
        dpddelay=35s

        ike=aes128gcm16-prfsha512-ecp256,aes128-sha2_512-prfsha512-ecp256,aes128-sha2_384-prfsha384-ecp256!
        esp=aes128gcm16-ecp256,aes128-sha2_512-prfsha512-ecp256!

        right={{ algo_vpn_host }}
        rightid={{ algo_vpn_host }}
        rightauth=pubkey

        # Change from Algo-generated version: allow any IP address
        # Upstream Algo only uses IP addresses and sets right=W.X.Y.Z,
        # but my fork can configure a domain name
        rightallowany=yes

        # Change from Algo-generated version: only use VPN for VPN hosts
        # aka "split-tunneling"
        # This means ONLY pass traffic destined for this network over the VPN
        rightsubnet=10.19.48.0/24

        # Change from Algo-generated version: restart dead peers
        dpdaction=restart

        # Change from Algo-generated version: keep trying to connect to the server forever
        keyingtries=%forever

        leftsourceip=%config
        leftauth=pubkey
        leftid=architect
        leftcert=architect.crt
        leftfirewall=yes
        left=%defaultroute

        # We don't use auto=route because it doesn't work with leftsourceip=%config
        # We use dbdaction=restart, keyingtries=%forever, and auto=start instead
        # See https://wiki.strongswan.org/issues/2162
        # TODO: I think this means we can get rid of our VPN keepalive pings?
        auto=start

# A host to ping periodically to keep the VPN tunnel up
# Can be anything that goes over the rightsubnet routes from roles/cfn_architect_ci/files/ipsec_architect.conf
vpn_keepalive_host: "architect.internal.micahrl.com"

# The CA cert from Algo
algo_cacert: |
    -----BEGIN CERTIFICATE-----
    MIIBwjCCAWegAwIBAgIJAIrq5lHj1GWaMAoGCCqGSM49BAMCMB4xHDAaBgNVBAMM
    E25ld3Ryb3kubWljYWhybC5jb20wHhcNMTgwNTA1MTk1OTE2WhcNMjgwNTAyMTk1
    OTE2WjAeMRwwGgYDVQQDDBNuZXd0cm95Lm1pY2FocmwuY29tMFkwEwYHKoZIzj0C
    AQYIKoZIzj0DAQcDQgAEajSK8iKvtQFheeIpsFv19jW1jqJBxzQ2zSkwuZQf0M1G
    F41K4q915Wg2lip5GMOR5949UEwphUQ7rGcj1ExEH6OBjTCBijAdBgNVHQ4EFgQU
    CGrXFKHGKHLrX2ZWF7AQsThIMaswTgYDVR0jBEcwRYAUCGrXFKHGKHLrX2ZWF7AQ
    sThIMauhIqQgMB4xHDAaBgNVBAMME25ld3Ryb3kubWljYWhybC5jb22CCQCK6uZR
    49RlmjAMBgNVHRMEBTADAQH/MAsGA1UdDwQEAwIBBjAKBggqhkjOPQQDAgNJADBG
    AiEAtioe8S83H2rohtjtGpuWkp9TrtdXONQfi4Wywg5ahCsCIQDCm9/WQV1j/0yH
    LOyD/wwFOIwDZew/9ZsacfMqMc8dsg==
    -----END CERTIFICATE-----

# The version of the architect-jenkins Docker image to use
# https://hub.docker.com/r/mrled/architect-jenkins/
archjenks_version: v0.0.3

# The version of the inflatable-wharf Docker image to use
# https://hub.docker.com/r/mrled/inflatable-wharf/
inflwharf_version: v0.0.6

# inflatable-wharf settings
# Used to do DNS challenges for Let's Encrypt
#
# The ID of the hosted zone
inflwharf_aws_hosted_zone_id: Z2OOB3F1JEVLME
# The region for the hosted zone (probably the same as your normal zone)
inflwharf_aws_region: "{{ aws_region }}"
# The domain name inflatable-wharf will request a cert for
inflwharf_domain_name: architect.internal.micahrl.com
# The email address we send to Let's Encrypt via inflatable-wharf
inflwharf_letsencrypt_email: psyops+architect@micahrl.com
# The Let's Encrypt server to use for generating the Architect TLS cert
# Use "staging" at first, to make sure everything works.
# This will prevent you from running into Let's Encrypt throttling,
# which can be quite severe (multiple days),
# but it will not generate certificates that are valid in browsers.
# Use "production" once everything works in staging;
# this is subject to harsh throttling, but will produce valid certs.
inflwharf_lets_encrypt_server: production

##
#### Values retrieved from the vault:
##

# Credentials used by this Ansible playbook
aws_access_key: "{{ vault_aws_access_key }}"
aws_secret_key: "{{ vault_aws_secret_key }}"

# inflatable-wharf access and secret key
# These credentials are separate from the deployment credentials (above)
# because these are permanently saved to your EC2 instance
# and therefore should have restricted settings to update your zone
inflwharf_aws_access_key: "{{ vault_inflwharf_aws_access_key }}"
inflwharf_aws_secret_key: "{{ vault_inflwharf_aws_secret_key }}"

# Get the user's VPN cert and private key, generated from Algo
# openssl pkcs12 -in "architect.p12" -clcerts -nokeys -passin pass:$ALGO_GENERATED_PKCS12PASS 2>/dev/null
# openssl pkcs12 -in "architect.p12" -nocerts -passin pass:$ALGO_GENERATED_PKCS12PASS -nodes 2>/dev/null
algo_vpn_cert: "{{ vault_algo_vpn_cert }}"
algo_vpn_key: "{{ vault_algo_vpn_key }}"

# Generate SSH client key
# Generate a client key that we can use to connect to the architect instance
# We do this ahead of time so that its saved to the repo, encrypted in the vault
# ssh-keygen -q -f ./architect.pem -N '' -t ecdsa && cat ./architect.pem | base64
architect_ssh_client_ecdsa_key_b64: "{{ vault_architect_ssh_client_ecdsa_key_b64 }}"

##
#### Variables probably not worth changing
##

# The workdir should not be changed, because it may be hardcoded elsewhere
workdir: workdir

# Workdir items:
known_hosts_file: "{{ workdir }}/known_hosts"
client_ssh_private_key_path: "{{ workdir }}/architect.pem"
client_ssh_public_key_path: "{{ client_ssh_private_key_path }}.pub"
ssh_script_path: "{{ workdir }}/ssh-architect.sh"

# CloudFormation stack names
architect_ci_stack_name: ArchitectCi

# Name of the admin user on the Architect EC2 instance
architect_user: architect

# Device name for the Jenkins data volume on the Architect EC2 instance
# Will create an EBS volume that is _not_ deleted on termination with this device name
architect_docker_volume_device: xvdj

# The mount point for the Jenkins data volume on the Architecft EC2 instance
architect_docker_volume_mountpoint: /var/lib/docker

# Common values we'll need to use for the Jenkins service,
# but that shouldn't need to be changed for different environments
architect_config_dir: /etc/architect
architect_jenkins_swarm_config_dir: "{{ architect_config_dir }}/jenkins-swarm"
architect_jenkins_swarm_compose_file: "{{ architect_jenkins_swarm_config_dir }}/architect.compose.yaml"
architect_jenkins_swarm_inflwharf_secrets_file: "{{ architect_jenkins_swarm_config_dir }}/inflwharf_secrets"
architect_jenkins_swarm_stack_name: architect-jenkins

# This should probably not be changed in this file,
# but overridden on the commandline with e.g. "-e remove_existing_architect_ci_stack=yes"
remove_existing_architect_ci_stack: no
skip_architect_ci_stack: no
skip_architect_docker_install: no
# This is set here for completeness, but see the deploy.yaml playbook
skip_architect_initialize_swarm: no
skip_architect_deploy_jenkins_swarm: no
