---

##
#### User-managed variables
##

# Your AWS region
aws_region: us-east-2

# The VPN host - either an IP address or a domain name
algo_vpn_host: newtroy.micahrl.com

# The parameterized contents of Algo's 'ipsec_USER.secrets' file:
algo_ipsec_secrets: "{{ algo_vpn_host }} : ECDSA architect.key"

# The contents of Algo's ipsec_USER.conf file
# We parameterize this where we can, and make a couple of changes, documented in the comments
algo_ipsec_conf: |
    conn ikev2-{{ algo_vpn_host }}
        fragmentation=yes
        rekey=no
        dpdaction=clear
        keyexchange=ikev2
        compress=no
        dpddelay=35s

        ike=aes128gcm16-prfsha512-ecp256,aes128-sha2_512-prfsha512-ecp256,aes128-sha2_384-prfsha384-ecp256!
        esp=aes128gcm16-ecp256,aes128-sha2_512-prfsha512-ecp256!

        right={{ algo_vpn_host }}
        rightid={{ algo_vpn_host }}
        rightauth=pubkey

        # Change from Algo-generated version: only use VPN for VPN hosts
        # aka "split-tunneling"
        # This means ONLY pass traffic destined for this network over the VPN
        rightsubnet=10.19.48.0/24,172.16.0.0/24

        leftsourceip=%config
        leftauth=pubkey
        leftid=architect
        leftcert=architect.crt
        leftfirewall=yes
        left=%defaultroute

        # Change from Algo-generated version:
        # start the tunnel when trying to route packets to 'rightsubnet'
        # We add a cron job to ping a host on the tunnel,
        # and this way we ensure the tunnel is always up
        # Better than auto=start, because if the tunnel goes down
        # (like if the VPN server reboots) auto=start will not restart it,
        # but auto=route will try again the next time packets are routed to it.
        # See also: https://wiki.strongswan.org/issues/1501s
        auto=route

# A host to ping periodically to keep the VPN tunnel up
# Can be anything that goes over the rightsubnet routes from roles/cfn_architect_ci/files/ipsec_architect.conf
vpn_keepalive_host: "architect.internal.micahrl.com"

# The CA cert from Algo run through base64: $(base64 cacert.pem)
algo_cacert_b64: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJ3akNDQVdlZ0F3SUJBZ0lKQUlycTVsSGoxR1dhTUFvR0NDcUdTTTQ5QkFNQ01CNHhIREFhQmdOVkJBTU0KRTI1bGQzUnliM2t1YldsallXaHliQzVqYjIwd0hoY05NVGd3TlRBMU1UazFPVEUyV2hjTk1qZ3dOVEF5TVRrMQpPVEUyV2pBZU1Sd3dHZ1lEVlFRRERCTnVaWGQwY205NUxtMXBZMkZvY213dVkyOXRNRmt3RXdZSEtvWkl6ajBDCkFRWUlLb1pJemowREFRY0RRZ0FFYWpTSzhpS3Z0UUZoZWVJcHNGdjE5alcxanFKQnh6UTJ6U2t3dVpRZjBNMUcKRjQxSzRxOTE1V2cybGlwNUdNT1I1OTQ5VUV3cGhVUTdyR2NqMUV4RUg2T0JqVENCaWpBZEJnTlZIUTRFRmdRVQpDR3JYRktIR0tITHJYMlpXRjdBUXNUaElNYXN3VGdZRFZSMGpCRWN3UllBVUNHclhGS0hHS0hMclgyWldGN0FRCnNUaElNYXVoSXFRZ01CNHhIREFhQmdOVkJBTU1FMjVsZDNSeWIza3ViV2xqWVdoeWJDNWpiMjJDQ1FDSzZ1WlIKNDlSbG1qQU1CZ05WSFJNRUJUQURBUUgvTUFzR0ExVWREd1FFQXdJQkJqQUtCZ2dxaGtqT1BRUURBZ05KQURCRwpBaUVBdGlvZThTODNIMnJvaHRqdEdwdVdrcDlUcnRkWE9OUWZpNFd5d2c1YWhDc0NJUURDbTkvV1FWMWovMHlICkxPeUQvd3dGT0l3RFpldy85WnNhY2ZNcU1jOGRzZz09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K

# The version of the architect-jenkins Docker image to use
# https://hub.docker.com/r/mrled/architect-jenkins/
archjenks_version: v0.0.3

# The version of the inflatable-wharf Docker image to use
# https://hub.docker.com/r/mrled/inflatable-wharf/
inflwharf_version: v0.0.3

# inflatable-wharf settings
# Used to do DNS challenges for Let's Encrypt
#
# The ID of the hosted zone
inflwharf_aws_hosted_zone_id: Z2OOB3F1JEVLME
# The region for the hosted zone (probably the same as your normal zone)
inflwharf_aws_region: "{{ aws_region }}"
# The domain name inflatable-wharf will request a cert for
inflwharf_domain_name: architect.internal.micahrl.com
# The email address we send to Let's Encrypt via inflatable-wharf
inflwharf_letsencrypt_email: psyops+architect@micahrl.com

##
#### Values retrieved from the vault:
##

# Credentials used by this Ansible playbook
aws_access_key: "{{ vault_aws_access_key }}"
aws_secret_key: "{{ vault_aws_secret_key }}"

# inflatable-wharf access and secret key
# These credentials are separate from the deployment credentials (above)
# because these are permanently saved to your EC2 instance
# and therefore should have restricted settings to update your zone
inflwharf_aws_access_key: "{{ vault_inflwharf_aws_access_key }}"
inflwharf_aws_secret_key: "{{ vault_inflwharf_aws_secret_key }}"

# Get the user's VPN cert and private key, generated from Algo
# openssl pkcs12 -in "/path/to/architect.p12" -clcerts -nokeys -passin pass:ALGO_GENERATED_PKCS12PASS 2>/dev/null | base64
# openssl pkcs12 -in "/path/to/architect.p12" -nocerts -passin pass:ALGO_GENERATED_PKCS12PASS -nodes 2>/dev/null | base64
algo_vpn_cert_b64: "{{ vault_algo_vpn_cert_b64 }}"
algo_vpn_key_b64: "{{ vault_algo_vpn_key_b64 }}"

# Generate SSH client key
# Generate a client key that we can use to connect to the architect instance
# We do this ahead of time so that its saved to the repo, encrypted in the vault
# ssh-keygen -q -f ./architect.pem -N '' -t ecdsa && cat ./architect.pem | base64
architect_ssh_client_ecdsa_key_b64: "{{ vault_architect_ssh_client_ecdsa_key_b64 }}"

# Generate SSH host key
# (We do this locally so that we can know the fingerprint)
# ssh-keygen -q -f ./ssh_host_ecdsa_key -N '' -t ecdsa && cat ./ssh_host_ecdsa_key | base64
architect_ssh_host_ecdsa_key_b64: "{{ vault_architect_ssh_host_ecdsa_key_b64 }}"

##
#### Variables probably not worth changing
##

# The workdir should not be changed, because it may be hardcoded elsewhere
workdir: workdir

# Workdir items:
known_hosts_file: "{{ workdir }}/known_hosts"
client_ssh_private_key_path: "{{ workdir }}/architect.pem"
client_ssh_public_key_path: "{{ client_ssh_private_key_path }}.pub"
ssh_script_path: "{{ workdir }}/ssh-architect.sh"

# CloudFormation stack names
architect_ci_stack_name: ArchitectCi
architect_kms_stack_name: ArchitectKms

# Name of the admin user on the Architect EC2 instance
architect_user: architect

# Device name for the Jenkins data volume on the Architect EC2 instance
# Will create an EBS volume that is _not_ deleted on termination with this device name
architect_docker_volume_device: xvdj

# The mount point for the Jenkins data volume on the Architecft EC2 instance
architect_docker_volume_mountpoint: /var/lib/docker

# Common values we'll need to use for the Jenkins service,
# but that shouldn't need to be changed for different environments
architect_config_dir: /etc/architect
architect_jenkins_swarm_config_dir: "{{ architect_config_dir }}/jenkins-swarm"
architect_jenkins_swarm_compose_file: "{{ architect_jenkins_swarm_config_dir }}/architect.compose.yaml"
architect_jenkins_swarm_inflwharf_secrets_file: "{{ architect_jenkins_swarm_config_dir }}/inflwharf_secrets"
architect_jenkins_swarm_stack_name: architect-jenkins

# This should probably not be changed in this file,
# but overridden on the commandline with e.g. "-e remove_existing_architect_ci_stack=yes"
remove_existing_architect_kms_stack: no
remove_existing_architect_ci_stack: no
skip_architect_kms_stack: no
skip_architect_ci_stack: no
skip_architect_docker_install: no
# This is set here for completeness, but see the deploy.yaml playbook
skip_architect_initialize_swarm: no
skip_architect_deploy_jenkins_swarm: no
